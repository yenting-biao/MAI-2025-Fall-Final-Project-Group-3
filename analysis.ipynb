{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e2fde1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9a82753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP_MAP = {\n",
    "    \"detectable_format:number_bullet_lists\": \"bullet_lists\",\n",
    "    \"length_constraints:number_words\": \"length_constraints\",\n",
    "    \"length_constraints:number_sentences\": \"length_constraints\",\n",
    "    \"length_constraints:number_paragraphs\": \"length_constraints\",\n",
    "    \"keywords:forbidden_words\": \"keywords\",\n",
    "    \"keywords:existence\": \"keywords\",\n",
    "    \"change_case:english_capital\": \"change_case\",\n",
    "    \"change_case:english_lowercase\": \"change_case\",\n",
    "    \"detectable_format:json_format\": \"json_format\",\n",
    "    \"startend:quotation\": \"wrapping\",\n",
    "    \"detectable_format:title\": \"wrapping\",\n",
    "    \"combination:repeat_prompt\": \"startend\",\n",
    "    \"startend:end_checker\": \"startend\",\n",
    "}\n",
    "GROUP_MAP = {k.replace(':', '_'): v for k, v in GROUP_MAP.items()}\n",
    "\n",
    "GROUP_MAP_CEQ = {\n",
    "    \"change_case:english_capital\": \"change_case\",\n",
    "    \"change_case:english_lowercase\": \"change_case\",\n",
    "    \"detectable_format:json_format\": \"json_format\",\n",
    "    \"startend:quotation\": \"wrapping\",\n",
    "    \"detectable_format:title\": \"wrapping\",\n",
    "    \"combination:repeat_prompt\": \"startend\",\n",
    "    \"startend:end_checker\": \"startend\",\n",
    "}\n",
    "GROUP_MAP_CEQ = {k.replace(':', '_'): v for k, v in GROUP_MAP_CEQ.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b3481c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rule_eval_jsonl(model_name:str, audio_task:str, response_task:str, IF_task:str, data_dir:str=\"model_responses\") -> dict[int, dict[str, Any]]:\n",
    "    IF_task = IF_task.replace(\":\", \"_\")\n",
    "    file_dir = os.path.join(data_dir, model_name, audio_task, response_task, IF_task, \"reports\")\n",
    "    files = os.listdir(file_dir)\n",
    "    files = [f for f in files if f.startswith(\"rule_eval@output_\") and f.endswith(\".jsonl\")]\n",
    "    results = {}\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(file_dir, file_name)\n",
    "        k = file_name.split(\"@output_\")[-1].split(\"-shot\")[0]\n",
    "        k = int(k)\n",
    "        results[k] = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                results[k].append(json.loads(line))\n",
    "    if len(results) != 9:\n",
    "        print(f\"Warning: Expected 9 shot levels, but got {len(results)} in {file_dir}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7ab31876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDFfromRuleEvalResults(results:dict[int, dict[str, Any]], response_task:str, performance_metric:str) -> pd.DataFrame:\n",
    "    d = {\"IF_task\": [], \"shot_level\": [], \"n\": []}\n",
    "    response_task = response_task.lower()\n",
    "    if response_task == \"creative_writing\":\n",
    "        d[\"if_rate\"] = []\n",
    "    elif response_task == \"close_ended_questions\":\n",
    "        d[\"if_rate_strict\"] = []\n",
    "        d[\"if_rate_loose\"] = []\n",
    "        d[\"mean_performance\"] = []\n",
    "    elif response_task == \"chain_of_thoughts\":\n",
    "        raise NotImplementedError(\"Chain of thoughts not implemented yet.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown response task: {response_task}\")\n",
    "\n",
    "    for k, v in results.items():\n",
    "        for shot_level, evaluations in v.items():\n",
    "            n = len(evaluations)\n",
    "            d[\"n\"].append(n)\n",
    "            d[\"IF_task\"].append(k)\n",
    "            d[\"shot_level\"].append(shot_level)\n",
    "\n",
    "            if response_task == \"creative_writing\":\n",
    "                follow_flags = [eval['follow_all_instructions'] for eval in evaluations]\n",
    "                d[\"if_rate\"].append(np.mean(follow_flags))\n",
    "            elif response_task == \"close_ended_questions\":\n",
    "                performance_scores = [eval[performance_metric] for eval in evaluations] if performance_metric in evaluations[0] else None\n",
    "                strict_follow_flags = [eval['strict_follow_all_instructions'] for eval in evaluations]\n",
    "                loose_follow_flags = [eval['loose_follow_all_instructions'] for eval in evaluations]\n",
    "                mean_performance = np.mean(performance_scores) if performance_scores is not None else None\n",
    "                if_rate_strict = np.mean(strict_follow_flags)\n",
    "                if_rate_loose = np.mean(loose_follow_flags)\n",
    "                d[\"if_rate_strict\"].append(if_rate_strict)\n",
    "                d[\"if_rate_loose\"].append(if_rate_loose)\n",
    "                d[\"mean_performance\"].append(mean_performance)\n",
    "\n",
    "    return pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0a4bbc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model_name:str, response_task:str=\"closed_ended_questions\", to_csv:bool=True):\n",
    "    response_task = response_task.lower()\n",
    "    d_df = {}\n",
    "    for audio_task, performance_metric in zip([\"ASR\", \"GR\", \"SER\"], [\"wer\", \"answer_correct\", \"answer_correct\"]):\n",
    "        IF_tasks = os.listdir(os.path.join(\"model_responses\", model_name, audio_task, response_task))\n",
    "        IF_tasks = [ft for ft in IF_tasks if ft in GROUP_MAP.keys()]\n",
    "        results = {}\n",
    "        for IF_task in IF_tasks:\n",
    "            results[IF_task] = get_rule_eval_jsonl(model_name, audio_task, response_task, IF_task)\n",
    "        d_df[audio_task] = createDFfromRuleEvalResults(results, response_task, performance_metric)\n",
    "\n",
    "    if to_csv:\n",
    "        for audio_task, df in d_df.items():\n",
    "            fn = os.path.join(\"./analysis\", f\"{model_name}_{audio_task}_{response_task}_summary.csv\")\n",
    "            print(f\"Saving summary to {fn}\")\n",
    "            df.to_csv(fn, index=False)\n",
    "\n",
    "    return d_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afedebd3",
   "metadata": {},
   "source": [
    "# close_ended_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5ad11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ qwen ------------\n",
      "Saving summary to ./analysis/qwen_ASR_closed_ended_questions_summary.csv\n",
      "Saving summary to ./analysis/qwen_GR_closed_ended_questions_summary.csv\n",
      "Saving summary to ./analysis/qwen_SER_closed_ended_questions_summary.csv\n",
      "------------ qwen2 ------------\n",
      "Saving summary to ./analysis/qwen2_ASR_closed_ended_questions_summary.csv\n",
      "Saving summary to ./analysis/qwen2_GR_closed_ended_questions_summary.csv\n",
      "Saving summary to ./analysis/qwen2_SER_closed_ended_questions_summary.csv\n",
      "------------ desta2_5 ------------\n",
      "Saving summary to ./analysis/desta2_5_ASR_closed_ended_questions_summary.csv\n",
      "Saving summary to ./analysis/desta2_5_GR_closed_ended_questions_summary.csv\n",
      "Saving summary to ./analysis/desta2_5_SER_closed_ended_questions_summary.csv\n",
      "------------ blsp-emo ------------\n",
      "Saving summary to ./analysis/blsp-emo_ASR_closed_ended_questions_summary.csv\n",
      "Saving summary to ./analysis/blsp-emo_GR_closed_ended_questions_summary.csv\n",
      "Saving summary to ./analysis/blsp-emo_SER_closed_ended_questions_summary.csv\n"
     ]
    }
   ],
   "source": [
    "response_task = \"closed_ended_questions\"\n",
    "d_df = {}\n",
    "for model_name in [\"qwen\", \"qwen2\", \"desta2_5\", \"blsp-emo\"]:\n",
    "    print(f\"------------ {model_name} ------------\")\n",
    "    d_df[model_name] = eval(model_name, response_task=response_task, to_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53607dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving combined summary to ./analysis/summary_ceq.xlsx\n"
     ]
    }
   ],
   "source": [
    "model_order = [\"qwen\", \"qwen2\", \"desta2_5\", \"blsp-emo\"]\n",
    "\n",
    "# put \"other\" last; keep the rest in a deterministic order\n",
    "group_order = []\n",
    "for v in GROUP_MAP_CEQ.values():\n",
    "    if v not in group_order and v != \"other\":\n",
    "        group_order.append(v)\n",
    "\n",
    "df_audio_task = {}\n",
    "fn = os.path.join(\"./analysis\", f\"summary_ceq.xlsx\")\n",
    "\n",
    "with pd.ExcelWriter(fn, engine=\"openpyxl\") as writer:\n",
    "    for audio_task in [\"ASR\", \"GR\", \"SER\"]:\n",
    "        dfs = []\n",
    "        for model_name in model_order:\n",
    "            df = d_df[model_name][audio_task].copy()\n",
    "\n",
    "            df[\"model\"] = model_name\n",
    "            df[\"IF_task_group\"] = df[\"IF_task\"].map(GROUP_MAP_CEQ)\n",
    "\n",
    "            df = df[[\n",
    "                \"IF_task_group\", \"IF_task\", \"n\", \"model\",\n",
    "                \"shot_level\", \"if_rate_strict\", \"if_rate_loose\", \"mean_performance\"\n",
    "            ]]\n",
    "            dfs.append(df)\n",
    "\n",
    "        df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        # --- ordered categoricals for deterministic sorting ---\n",
    "        # shot_level: if it's numeric, this works; if it's strings like \"0-shot\", see note below\n",
    "        shot_order = sorted(df_all[\"shot_level\"].dropna().unique())\n",
    "        df_all[\"shot_level\"] = pd.Categorical(df_all[\"shot_level\"], categories=shot_order, ordered=True)\n",
    "\n",
    "        df_all[\"IF_task_group\"] = pd.Categorical(df_all[\"IF_task_group\"], categories=group_order, ordered=True)\n",
    "        df_all[\"model\"] = pd.Categorical(df_all[\"model\"], categories=model_order, ordered=True)\n",
    "\n",
    "        df_all_compare_shots = df_all.sort_values(\n",
    "            by=[\"IF_task_group\", \"IF_task\", \"model\"],\n",
    "            ascending=[True, True, True],\n",
    "            kind=\"mergesort\",\n",
    "        )\n",
    "\n",
    "        df_all = df_all.sort_values(\n",
    "            by=[\"shot_level\", \"IF_task_group\", \"IF_task\", \"model\"],\n",
    "            ascending=[True, True, True, True],\n",
    "            kind=\"mergesort\",\n",
    "        )\n",
    "\n",
    "        df_all_grouped = df_all.groupby([\"shot_level\", \"IF_task_group\", \"model\"], observed=False).mean(numeric_only=True).reset_index()\n",
    "        df_all_compare_shots_grouped = df_all_compare_shots.groupby([\"IF_task_group\", \"model\", \"shot_level\"], observed=False).mean(numeric_only=True).reset_index()\n",
    "\n",
    "        df_all.to_excel(writer, sheet_name=audio_task, index=False)\n",
    "        df_all_grouped.to_excel(writer, sheet_name=f\"{audio_task}_grouped\", index=False)\n",
    "        df_all_compare_shots.to_excel(writer, sheet_name=f\"{audio_task}_compare_shots\", index=False)\n",
    "        df_all_compare_shots_grouped.to_excel(writer, sheet_name=f\"{audio_task}_compare_shots_grouped\", index=False)\n",
    "\n",
    "        df_audio_task[audio_task] = df_all\n",
    "        df_audio_task[f\"{audio_task}_grouped\"] = df_all_grouped\n",
    "        df_audio_task[f\"{audio_task}_compare_shots\"] = df_all_compare_shots\n",
    "        df_audio_task[f\"{audio_task}_compare_shots_grouped\"] = df_all_compare_shots_grouped\n",
    "\n",
    "print(f\"Saving combined summary to {fn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "eca12877",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ASR', 'ASR_grouped', 'ASR_compare_shots', 'ASR_compare_shots_grouped', 'GR', 'GR_grouped', 'GR_compare_shots', 'GR_compare_shots_grouped', 'SER', 'SER_grouped', 'SER_compare_shots', 'SER_compare_shots_grouped'])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_audio_task.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641011b0",
   "metadata": {},
   "source": [
    "# creative_writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ac241724",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ qwen ------------\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_responses/qwen/ASR/creative_writing/keywords_forbidden_words/reports'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[160]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mqwen\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mqwen2\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdesta2_5\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mblsp-emo\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m------------ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ------------\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     d_df[model_name] = \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_task\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[159]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36meval\u001b[39m\u001b[34m(model_name, response_task, to_csv)\u001b[39m\n\u001b[32m      7\u001b[39m     results = {}\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m IF_task \u001b[38;5;129;01min\u001b[39;00m IF_tasks:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         results[IF_task] = \u001b[43mget_rule_eval_jsonl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_task\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_task\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIF_task\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     d_df[audio_task] = createDFfromRuleEvalResults(results, response_task, performance_metric)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m to_csv:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[152]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mget_rule_eval_jsonl\u001b[39m\u001b[34m(model_name, audio_task, response_task, IF_task, data_dir)\u001b[39m\n\u001b[32m      2\u001b[39m IF_task = IF_task.replace(\u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m file_dir = os.path.join(data_dir, model_name, audio_task, response_task, IF_task, \u001b[33m\"\u001b[39m\u001b[33mreports\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m files = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m files = [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files \u001b[38;5;28;01mif\u001b[39;00m f.startswith(\u001b[33m\"\u001b[39m\u001b[33mrule_eval@output_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m f.endswith(\u001b[33m\"\u001b[39m\u001b[33m.jsonl\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m      6\u001b[39m results = {}\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'model_responses/qwen/ASR/creative_writing/keywords_forbidden_words/reports'"
     ]
    }
   ],
   "source": [
    "response_task = \"creative_writing\"\n",
    "d_df_cw = {}\n",
    "for model_name in [\"qwen\", \"qwen2\", \"desta2_5\", \"blsp-emo\"]:\n",
    "    print(f\"------------ {model_name} ------------\")\n",
    "    d_df[model_name] = eval(model_name, response_task=response_task, to_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f89c2f0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech-ifeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
