{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fde1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3481c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rule_eval_jsonl(model_name:str, audio_task:str, response_task:str, IF_task:str, data_dir:str=\"model_responses\") -> dict[int, dict[str, Any]]:\n",
    "    IF_task = IF_task.replace(\":\", \"_\")\n",
    "    file_dir = os.path.join(data_dir, model_name, audio_task, response_task, IF_task, \"reports\")\n",
    "    files = os.listdir(file_dir)\n",
    "    files = [f for f in files if f.startswith(\"rule_eval@output_\") and f.endswith(\".jsonl\")]\n",
    "    results = {}\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(file_dir, file_name)\n",
    "        k = file_name.split(\"@output_\")[-1].split(\"-shot\")[0]\n",
    "        k = int(k)\n",
    "        results[k] = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                results[k].append(json.loads(line))\n",
    "    if len(results) != 9:\n",
    "        print(f\"Warning: Expected 9 shot levels, but got {len(results)} in {file_dir}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ab31876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDFfromRuleEvalResults(results:dict[int, dict[str, Any]], performance_metric:str) -> pd.DataFrame:\n",
    "    d = {\n",
    "        \"IF_task\": [],\n",
    "        \"shot_level\": [],\n",
    "        \"if_rate_strict\": [],\n",
    "        \"if_rate_loose\": [],\n",
    "        \"mean_performance\": [],\n",
    "        \"n\": []\n",
    "    }\n",
    "\n",
    "    for k, v in results.items():\n",
    "        for shot_level, evaluations in v.items():\n",
    "            performance_scores = [eval[performance_metric] for eval in evaluations] if performance_metric in evaluations[0] else None\n",
    "            strict_follow_flags = [eval['strict_follow_all_instructions'] for eval in evaluations]\n",
    "            loose_follow_flags = [eval['loose_follow_all_instructions'] for eval in evaluations]\n",
    "            mean_performance = np.mean(performance_scores) if performance_scores is not None else None\n",
    "            if_rate_strict = np.mean(strict_follow_flags)\n",
    "            if_rate_loose = np.mean(loose_follow_flags)\n",
    "            n = len(evaluations)\n",
    "            d[\"IF_task\"].append(k)\n",
    "            d[\"shot_level\"].append(shot_level)\n",
    "            d[\"if_rate_strict\"].append(if_rate_strict)\n",
    "            d[\"if_rate_loose\"].append(if_rate_loose)\n",
    "            d[\"mean_performance\"].append(mean_performance)\n",
    "            d[\"n\"].append(n)\n",
    "\n",
    "    return pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0a4bbc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model_name:str, to_csv:bool=True):\n",
    "    response_task = \"closed_ended_questions\"\n",
    "\n",
    "    d_df = {}\n",
    "    for audio_task, performance_metric in zip([\"ASR\", \"GR\", \"SER\"], [\"wer\", \"answer_correct\", \"answer_correct\"]):\n",
    "        IF_tasks = os.listdir(os.path.join(\"model_responses\", model_name, audio_task, response_task))\n",
    "        results = {}\n",
    "        for IF_task in IF_tasks:\n",
    "            results[IF_task] = get_rule_eval_jsonl(model_name, audio_task, response_task, IF_task)\n",
    "        d_df[audio_task] = createDFfromRuleEvalResults(results, performance_metric)\n",
    "\n",
    "    if to_csv:\n",
    "        for audio_task, df in d_df.items():\n",
    "            fn = os.path.join(\"./analysis\", f\"{model_name}_{audio_task}_{response_task}_summary.csv\")\n",
    "            print(f\"Saving summary to {fn}\")\n",
    "            df.to_csv(fn, index=False)\n",
    "\n",
    "    return d_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4a5ad11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ qwen ------------\n",
      "Saving summary to ./analysis/qwen_ASR_closed_ended_questions_summary.csv\n",
      "Saving summary to ./analysis/qwen_GR_closed_ended_questions_summary.csv\n",
      "Saving summary to ./analysis/qwen_SER_closed_ended_questions_summary.csv\n",
      "------------ qwen2 ------------\n",
      "Saving summary to ./analysis/qwen2_ASR_closed_ended_questions_summary.csv\n",
      "Saving summary to ./analysis/qwen2_GR_closed_ended_questions_summary.csv\n",
      "Saving summary to ./analysis/qwen2_SER_closed_ended_questions_summary.csv\n",
      "------------ desta2_5 ------------\n",
      "Saving summary to ./analysis/desta2_5_ASR_closed_ended_questions_summary.csv\n",
      "Saving summary to ./analysis/desta2_5_GR_closed_ended_questions_summary.csv\n",
      "Saving summary to ./analysis/desta2_5_SER_closed_ended_questions_summary.csv\n",
      "------------ blsp-emo ------------\n",
      "Saving summary to ./analysis/blsp-emo_ASR_closed_ended_questions_summary.csv\n",
      "Saving summary to ./analysis/blsp-emo_GR_closed_ended_questions_summary.csv\n",
      "Saving summary to ./analysis/blsp-emo_SER_closed_ended_questions_summary.csv\n"
     ]
    }
   ],
   "source": [
    "d_df = {}\n",
    "for model_name in [\"qwen\", \"qwen2\", \"desta2_5\", \"blsp-emo\"]:\n",
    "    print(f\"------------ {model_name} ------------\")\n",
    "    d_df[model_name] = eval(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cef467",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_map = {\n",
    "    \"detectable_format:number_bullet_lists\": \"bullet_lists\",\n",
    "    \"length_constraints:number_words\": \"length_constraints\",\n",
    "    \"length_constraints:number_sentences\": \"length_constraints\",\n",
    "    \"length_constraints:number_paragraphs\": \"length_constraints\",\n",
    "    \"keywords:forbidden_words\": \"keywords\",\n",
    "    \"keywords:existence\": \"keywords\",\n",
    "    \"change_case:english_capital\": \"change_case\",\n",
    "    \"change_case:english_lowercase\": \"change_case\",\n",
    "    \"detectable_format:json_format\": \"json_format\",\n",
    "    \"startend:quotation\": \"wrapping\",\n",
    "    \"detectable_format:title\": \"wrapping\",\n",
    "    \"combination:repeat_prompt\": \"startend\",\n",
    "    \"startend:end_checker\": \"startend\",\n",
    "}\n",
    "group_map = {k.replace(':', '_'): v for k, v in group_map.items()}\n",
    "\n",
    "group_map_ceq = {\n",
    "    \"change_case:english_capital\": \"change_case\",\n",
    "    \"change_case:english_lowercase\": \"change_case\",\n",
    "    \"detectable_format:json_format\": \"json_format\",\n",
    "    \"startend:quotation\": \"wrapping\",\n",
    "    \"detectable_format:title\": \"wrapping\",\n",
    "    \"combination:repeat_prompt\": \"startend\",\n",
    "    \"startend:end_checker\": \"startend\",\n",
    "}\n",
    "group_map_ceq = {k.replace(':', '_'): v for k, v in group_map_ceq.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "53607dc6",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving combined summary to ./analysis/summary_ceq.xlsx\n"
     ]
    }
   ],
   "source": [
    "model_order = [\"qwen\", \"qwen2\", \"desta2_5\", \"blsp-emo\"]\n",
    "\n",
    "# put \"other\" last; keep the rest in a deterministic order\n",
    "group_order = []\n",
    "for v in group_map_ceq.values():\n",
    "    if v not in group_order and v != \"other\":\n",
    "        group_order.append(v)\n",
    "\n",
    "df_audio_task = {}\n",
    "fn = os.path.join(\"./analysis\", f\"summary_ceq.xlsx\")\n",
    "\n",
    "with pd.ExcelWriter(fn, engine=\"openpyxl\") as writer:\n",
    "    for audio_task in [\"ASR\", \"GR\", \"SER\"]:\n",
    "        dfs = []\n",
    "        for model_name in model_order:\n",
    "            df = d_df[model_name][audio_task].copy()\n",
    "\n",
    "            df[\"model\"] = model_name\n",
    "            df[\"IF_task_group\"] = df[\"IF_task\"].map(group_map_ceq)\n",
    "\n",
    "            df = df[[\n",
    "                \"IF_task_group\", \"IF_task\", \"n\", \"model\",\n",
    "                \"shot_level\", \"if_rate_strict\", \"if_rate_loose\", \"mean_performance\"\n",
    "            ]]\n",
    "            dfs.append(df)\n",
    "\n",
    "        df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        # --- ordered categoricals for deterministic sorting ---\n",
    "        # shot_level: if it's numeric, this works; if it's strings like \"0-shot\", see note below\n",
    "        shot_order = sorted(df_all[\"shot_level\"].dropna().unique())\n",
    "        df_all[\"shot_level\"] = pd.Categorical(df_all[\"shot_level\"], categories=shot_order, ordered=True)\n",
    "\n",
    "        df_all[\"IF_task_group\"] = pd.Categorical(df_all[\"IF_task_group\"], categories=group_order, ordered=True)\n",
    "        df_all[\"model\"] = pd.Categorical(df_all[\"model\"], categories=model_order, ordered=True)\n",
    "\n",
    "        # stable sort so ties keep predictable order\n",
    "        df_all = df_all.sort_values(\n",
    "            by=[\"shot_level\", \"IF_task_group\", \"IF_task\", \"model\"],\n",
    "            ascending=[True, True, True, True],\n",
    "            kind=\"mergesort\",\n",
    "        )\n",
    "\n",
    "        df_all.to_excel(writer, sheet_name=audio_task, index=False)\n",
    "        df_audio_task[audio_task] = df_all\n",
    "\n",
    "        df_audio_task[f\"{audio_task}_grouped\"] = df_all.groupby([\"shot_level\", \"IF_task_group\", \"model\"], observed=False).mean(numeric_only=True).reset_index()\n",
    "        df_audio_task[f\"{audio_task}_grouped\"].to_excel(writer, sheet_name=f\"{audio_task}_grouped\", index=False)\n",
    "\n",
    "print(f\"Saving combined summary to {fn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "eca12877",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ASR', 'ASR_grouped', 'GR', 'GR_grouped', 'SER', 'SER_grouped'])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_audio_task.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech-ifeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
